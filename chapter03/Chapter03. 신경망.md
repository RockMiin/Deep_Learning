## Chapter03. 신경망

컴퓨터가 수행하는 복잡한 처리도 퍼셉트론으로 이론상 표현할 수 있다. 하지만 **가중치를 설정하는 작업은 여전히 사람이 수동**으로 한다.

신경망은 **가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력**을 가지고 있다.

**신경망**

신경망은 퍼셉트론과 공통점이 많지만 이번 장에서는 다른 점을 중심으로 신경망의 구조를 설명한다.

신경망은 입력층, 은닉층, 출력층으로 구성되어 있다.

활성화 함수는 임계값을 경계로 출력이 바뀌는데, 이러한 함수를 **계단함수**라고 한다. 그래서 퍼셉트론에서는 활성화 함수로 계단 함수를 이용한다라고 할 수 있다. 활성화 함수를 계단 함수 이외의 함수를 사용하는 것이 신경망의 세계를 나아가는 열쇠이다.

**계단함수**

```python
def step_function(x):
    if x>0:
        return 1
    else:
        return 0
```

위 구현은 인수 x는 실수만 받아들인다는 단점이 있기 때문에 numpy 배열도 지원하도록 구현해보자.

```python
import numpy as np
def step_function(x):
    y=x>0
    return y.astype(np.int)
```

계단 함수의 그래프

```python
import numpy as np
import matplotlib.pylab as plt

def step_function(x):
    return np.array(x>0, dtype=np.int)

x=np.arange(-5.0, 5.0, 0.1)
y= step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```



**시그모이드 함수**

h(x)= 1/ (1+exp(-x))

얼핏 복잡해 보이지만 이 역시 단순한 '함수'이다. 입력을 주면 출력을 돌려주는 변환기이다.

```python
def sigmoid(x):
    return 1/ (1+np.exp(-x))

x= np.arange(-5.0, 5.0, 0.1)
y=sigmoid(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```



**시그모이드 함수와 계단 함수 비교**

시그모이드 함수 :  부드러운 곡선이며 입력에 따라 출력이 **연속적**으로 변함

계단 함수 : 0을 경계로 출력이 갑자기 바뀌어림

하지만 두 함수 모두 입력이 작을 때의 출력은 0에 가깝고 입력이 커지면 출력이 1에 가까워지는 구조이다. 또한 입력에 상관없이 출력은 0에서 1 사이이다. 중요한 공통점은, 두 함수 모두 **비선형 함수**이다.



신경망에서는 활성화 함수로 비선형 함수를 사용해야 한다. 왜냐하면 선형 함수를 이용하면 신경망의 층을 깊게 하는 의미가 없기 때문이다. 층을 쌓는 혜택을 얻고 싶다면 활성화 함수로는 반드시 비선형 함수를 사용해야 한다.



**ReLU 함수**

```python
def relu(x):
    return np.maximum(0, x)
```

